!!! 6장 
결정트리

과대적합

불순도 - 색깔 반반 , 불순도가 최저값 = 반반, 색깔 3개 = 최대값
        불순도로 하이퍼파라미터 지정, 불순도 = 최대 = 분류 x
        숫자가 낮을수록 굿

엔트로피 - 불순도=엔트로피 가튼 개념 ( 50:50, 0, 0.5 - 불순도 , 엔트로피는 0,1 )
          값을 한 번더 곱해야함 - 값의 표현의 차이, 범위의 차이

결정트리 알고리즘 - 엔트로피, 불순도를 0 (최저)에 가깝게 학습하는 것이 목표 - 과대적합 주의
                  무조건 쉽고 편함 >> 사람이 편한 것을 말함. 이해하기 편함. (학습을 하는 것에 대한 장점은 x
                  무식하게 단순히 쪼개는 거라서 효율은 xxxx 

화이트박스 - 결정트리 유리  ( 나머지는 블랙박스)
>> why 화이트박스 모델은 줄어들엇디예? 
        직관적 이해가 가능해서예, 신경망 내부로 돌아가는 과정 - 블랙박스
>> why 화이트박스는 왜 결정트리만 ?
        쉬움 , but 데이터 학습이 핵심이라서 쉬운건 중요x , 학습을 시켜서 좋은 데이터를 만드는 것이 더 굳
        좋다는 기분이 명확 X
>> 사람이 이해하기 쉽다~ 


(규제 매개변수 안함)



회귀 -   한 번 쪼개고 값들을 중심으로 한 번더 쪼갬 -- 계속 쪼갬 -- 이것이 식의 형태로 됨.
        결정트리에서 회귀가 O, but 사용은 x (회귀에서) ,과대적합 확률 ^

축 방향에 대한 민감성 
- 같은 값 = 변형이 생겨도 비슷함 >> but 같은 값으로 축 방향 회전 시 복잡해짐. (비효율)
결정 트리의 분산 문제 




!!! 7장
앙상블 학습 - 최적만 x , 여러 개의 알고리즘을 활용하자! (잘 섞어서 ❤)
투표 기반 분류기 
> 직접 투표 분류기 : 다수결 투표로 알고리즘 선택 ( 31p )
> 좋은거 투표로 선택하깅

배깅과 페이스팅 - 데이터가 10만 개가 잇고 분류해서 넣을겨

페이스팅 - 2만 5천개씩 중복 x, 샘플링 >> 모든 데이터 학습 O
배깅 -  중복 허용, 데이터 랜덤으로 돌림 >> 모든 데이터 학습 x
>> 오히려 결정트리가 부드럽게 O 

배깅 <-> OOB 평가
: 수학적으로 살써지는 데이터가 37% , OOB 샘플을 테스트 돌림 >> 배깅 이후 OOB 스코어까지 냄 (바로 정확도 출력)

랜덤 포레스트 - 500개씩 나눠서 각각 학습 후 (랜덤) 나중에 합침~( 특성중요도)

부스팅 - 약한 학습기 -- 여러 개 연결하여 강한 학습기를 만드는 앙상블 학습
> AdaBoost(보통 쓰이는거) - 5개의 학습기 돌림 > 다 다른 데이터 내놓아줌 ~ (5개의 특징이 잇음 >> 점점 하나로 모으게 되)

------------일반 앙상블 ^ 
 
스태킹 - 마지막에 한 번 더 재훈련 (블렌더 / 좋아질수도 잇고 아닐수도 잇음)
>> 앙상블 >> 한 번더 학습 >> 결과 

실습 개크게 참조하기 




!!! 8장

( 쳐 잠 병신이 )



!!! 9장
