!!! 6장 
결정트리

과대적합

불순도 - 색깔 반반 , 불순도가 최저값 = 반반, 색깔 3개 = 최대값
        불순도로 하이퍼파라미터 지정, 불순도 = 최대 = 분류 x
        숫자가 낮을수록 굿

엔트로피 - 불순도=엔트로피 가튼 개념 ( 50:50, 0, 0.5 - 불순도 , 엔트로피는 0,1 )
          값을 한 번더 곱해야함 - 값의 표현의 차이, 범위의 차이

결정트리 알고리즘 - 엔트로피, 불순도를 0 (최저)에 가깝게 학습하는 것이 목표 - 과대적합 주의
                  무조건 쉽고 편함 >> 사람이 편한 것을 말함. 이해하기 편함. (학습을 하는 것에 대한 장점은 x
                  무식하게 단순히 쪼개는 거라서 효율은 xxxx 

화이트박스 - 결정트리 유리  ( 나머지는 블랙박스)
>> why 화이트박스 모델은 줄어들엇디예? 
        직관적 이해가 가능해서예, 신경망 내부로 돌아가는 과정 - 블랙박스
>> why 화이트박스는 왜 결정트리만 ?
        쉬움 , but 데이터 학습이 핵심이라서 쉬운건 중요x , 학습을 시켜서 좋은 데이터를 만드는 것이 더 굳
        좋다는 기분이 명확 X
>> 사람이 이해하기 쉽다~ 


(규제 매개변수 안함)



회귀 -   한 번 쪼개고 값들을 중심으로 한 번더 쪼갬 -- 계속 쪼갬 -- 이것이 식의 형태로 됨.
        결정트리에서 회귀가 O, but 사용은 x (회귀에서) ,과대적합 확률 ^

축 방향에 대한 민감성 
- 같은 값 = 변형이 생겨도 비슷함 >> but 같은 값으로 축 방향 회전 시 복잡해짐. (비효율)
결정 트리의 분산 문제 




!!! 7장
앙상블 학습 - 최적만 x , 여러 개의 알고리즘을 활용하자! (잘 섞어서 ❤)
투표 기반 분류기 
> 직접 투표 분류기 : 다수결 투표로 알고리즘 선택 ( 31p )
> 좋은거 투표로 선택하깅

배깅과 페이스팅 - 데이터가 10만 개가 잇고 분류해서 넣을겨

페이스팅 - 2만 5천개씩 중복 x, 샘플링 >> 모든 데이터 학습 O
배깅 -  중복 허용, 데이터 랜덤으로 돌림 >> 모든 데이터 학습 x
>> 오히려 결정트리가 부드럽게 O 

배깅 <-> OOB 평가
: 수학적으로 살써지는 데이터가 37% , OOB 샘플을 테스트 돌림 >> 배깅 이후 OOB 스코어까지 냄 (바로 정확도 출력)

랜덤 포레스트 - 500개씩 나눠서 각각 학습 후 (랜덤) 나중에 합침~( 특성중요도)

부스팅 - 약한 학습기 -- 여러 개 연결하여 강한 학습기를 만드는 앙상블 학습
> AdaBoost(보통 쓰이는거) - 5개의 학습기 돌림 > 다 다른 데이터 내놓아줌 ~ (5개의 특징이 잇ㄴ음 >> 점점 하나로 모으게 되)

------------일반 앙상블 ^ 
 
스태킹 - 마지막에 한 번 더 재훈련 (블렌더 / 좋아질수도 잇고 아닐수도 잇음)
>> 앙상블 >> 한 번더 학습 >> 결과 

실습 개크게 참조하기 




!!! 8장

차원의 저주 - 훈련 샘플 각각이 수천 - 수백만 개의 특성을 가짐 >> 많은 특성을 후년을 느리게 + 솔루션 찾기 X
             3차원 세계에서 고차원 공간은 직관적 상상 X
                : 훈련 세트의 차원이 클수록 과대적합 위험 ^
차원 축소 접근법
- 투영 : 모든 훈련 샘플이 고차원 공간 안의 저차원 부분 공간에 O
        >> 3D - 2D 축소 

>>> 55P 공부해보기 ㅅㅂ <<<

-스위스 롤 데이터셋
        :부분 공간이 뒤틀리거나 휨.
        :스위스 롤을 펼쳐서 >> 2D 데이터셋 구함.

- 매니폴드 학습
        : d차원 매니폴드
                국부적으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부 ( d < n )
                 스위스 롤의 경우에는 d=2고 n=3

        : 매니폴드 학습
                매니폴드 가정 또는 매니폴드 가설
                ( 대ㅜ부분 실제 고차원 데이터셋이 더 낮은 저차원 매니폴드에 가깝게 놓여 있다는 가설
                >> 암묵적으로 다른 가정과 병행 - 항상 유효 X

주성분 분석 - PCA는 훈련 세트에서 분산이 최대인 축 find
             고차원 데이터셋이라면 PCA는 이전의 두 축에 직교하는 세 번째 축을 찾으며 데이터셋에 있는 차원의 수만큼 n,n+1 .... 번째 축을 찾음
                >> i번재 축을 이 데이터에 i번째 주성분이라캄

                : 넘파이의 svd() 함수를 사용 >> 3d 훈련 세트의 모든 주성분을 구함 >> 두 개의 PC를 정의하는 두 개의 단위 벡터 추출
                : 사이킷런 사용 - PCA 모델 사용 - 데이터셋의 차원을 2로 줄이는 코드
                : 설명된 분산의 비율 - 3D 데이터셋의 처음 두 주성분에 대한 설명된 분산 비율
                : 적절한 차원 수 선택 - MNIST 데이터셋 로드 - 분할 >> 차원 줄이지 x  >> PCA 수행
                : n_components = d로 설정 >> PCA 다시 실행 - 실제 주성분 개수는 훈련 중 결정 >> n_components_ 속성에 저장
                : PCA 사용 > 차원 줄인 후 >> 랜덤 포레스트<< 사용, 분류 수행 
                        ( RandomizedSearchCV를 사용 > PCA와 랜덤 포레스트 분류기에 잘맞는 하이퍼파라미터 조합 ㄱ)
           - 압축을 위한 PCA 
                : 차원 축소 후 훈련 세트 >> 작은 공간  ( 데이터셋 크기 원본의 20% 이하 >> 분산 5%만 손실)
                : 압축된 데이터셋에 PCA 투영의 변환을 반대로 적용 >> 784개의 차원으로 되돌릴 수 O
                ( 투영에서 일정량의 정도<유실된 5%의 분산>을 잃어버렸기 때문 >> 원본 데이터셋 얻기 x )
                : 재구성 오차 (reconstruction error)
                ( 원본 데이터와 재구성된 데이터 (압축 후 원상 복구) 사이의 평균 제곱 거리
                : inverse_transform() 메서드를 사용 > 축소된 MNIST 데이터 집합 다시 784개 차원으로 복원
          - 랜덤 PCA





!!! 9장
